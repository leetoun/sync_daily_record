原始数据是28*28 1：数据层： layer { name: "mnist"//数据层的名字是mnist type: "Data"//这个层的类型是data top: "data"//产生两个blob，一个是data blob top: "label"//一个是lable blob include { phase: TRAIN } transform_param { scale: 0.00390625//像素归一化 } data_param { source: "examples/mnist/mnist_train_lmdb" batch_size: 64 backend: LMDB } } 2：卷积层 layer { name: "conv1" type: "Convolution" bottom: "data"//获取上一层的data blob top: "conv1"//产生conv1层 param { lr_mult: 1//学习率。表示 weight的学习率和slover.pro中的学习率是一致的。 } param { lr_mult: 2//表示 bias的学习率是slover.pro中的学习率的2倍。  这样设置会导致更快的收敛 } convolution_param { num_output: 20//cov1层将产生输出20个通道 kernel_size: 5//卷积核大小是5*5 stride: 1//步长是1 weight_filler {//权重填充器，使用xavier算法填充weight。根据输入和输出神经元的数量自动确定初始化的规模。 type: "xavier" } bias_filler {//偏置填充器，使用constant算法填充bias。是一个常数，默认是0 type: "constant" } } } 3：池化层(避免数据过拟合) layer { name: "pool1" type: "Pooling" bottom: "conv1" top: "pool1" pooling_param { pool: MAX//使用MAX进行池化 kernel_size: 2//卷积核大小是2*2 stride: 2//步长是2 } } 4：全连接层 layer { name: "ip1" type: "InnerProduct" bottom: "pool2" top: "ip1" param { lr_mult: 1 } param { lr_mult: 2 } inner_product_param { num_output: 500//产生500维的输出数据 weight_filler { type: "xavier" } bias_filler { type: "constant" } } } 5：ReLU层（紧跟在全连接层后，目的是节省内存） layer { name: "relu1" type: "ReLU" bottom: "ip1" top: "ip1" } ReLU层后紧跟一个InnerProduct层 layer { name: "ip2" type: "InnerProduct" bottom: "ip1" top: "ip2" param { lr_mult: 1 } param { lr_mult: 2 } inner_product_param { num_output: 10//因为有10类，所以输出10 weight_filler { type: "xavier" } bias_filler { type: "constant" } } } 6：Loss层//不产生任何输出，只是用来计算损失函数的值，用来初始化ip2的gradient layer { name: "loss" type: "SoftmaxWithLoss" bottom: "ip2"//需要两个blob,一个是ip2，作为预测用 bottom: "label"//来自数据层，作为标签 top: "loss" }
--------------------- 
作者：wjmishuai 
来源：CSDN 
原文：https://blog.csdn.net/wjmishuai/article/details/50890214 
版权声明：本文为博主原创文章，转载请附上博文链接！
